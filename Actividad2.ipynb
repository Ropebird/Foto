{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ropebird/Foto/blob/main/Actividad2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar PySpark y dependencias de Java si no están ya instaladas\n",
        "!pip install pyspark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Establecer la variable de entorno para Java\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tACv3SmL2CqV",
        "outputId": "f5c2ccd9-3d36-469b-d906-9e65ddaab5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "gNT6vBFl2C5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar el conector de Hadoop para GCS\n",
        "!wget -q https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\n",
        "\n",
        "# Configurar SparkSession con soporte para GCS\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GCS_Spark\") \\\n",
        "    .config(\"spark.jars\", \"gcs-connector-hadoop3-latest.jar\") \\\n",
        "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
        "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/content/key.json\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "TWtzieVL2Hjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar e instalar Kafka en Google Colab\n",
        "!wget -qO - https://archive.apache.org/dist/kafka/3.5.1/kafka_2.12-3.5.1.tgz | tar -xz\n",
        "\n",
        "# Definir el directorio de Kafka\n",
        "KAFKA_DIR = \"/content/kafka_2.12-3.5.1\"\n",
        "\n",
        "# Iniciar Zookeeper (necesario para Kafka)\n",
        "!$KAFKA_DIR/bin/zookeeper-server-start.sh -daemon $KAFKA_DIR/config/zookeeper.properties\n",
        "\n",
        "# Iniciar el servidor Kafka\n",
        "!$KAFKA_DIR/bin/kafka-server-start.sh -daemon $KAFKA_DIR/config/server.properties\n",
        "\n",
        "# Esperar unos segundos para que Kafka arranque\n",
        "import time\n",
        "time.sleep(10)\n",
        "\n",
        "print(\"✅ Kafka y Zookeeper están en ejecución\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjW4snqhxA52",
        "outputId": "a99b1fad-30c4-4c88-b67a-bdb1c05e2b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Kafka y Zookeeper están en ejecución\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Iniciar la sesión de Spark con el conector de Kafka\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KafkaStreaming\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"✅ Sesión de Spark creada con Kafka\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6GsCuarxezn",
        "outputId": "c65ed4b2-3abe-408c-e6aa-03bcdd09c686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sesión de Spark creada con Kafka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reemplaza la URL con la del archivo que quieres descargar\n",
        "!wget -O flights.csv https://raw.githubusercontent.com/Ropebird/Tarea-Kafka/refs/heads/main/flights.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdMyJfIq_soA",
        "outputId": "3725d2da-300f-4527-f296-cc3e70af9431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-24 22:25:34--  https://raw.githubusercontent.com/Ropebird/Tarea-Kafka/refs/heads/main/flights.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10033769 (9.6M) [text/plain]\n",
            "Saving to: ‘flights.csv’\n",
            "\n",
            "flights.csv         100%[===================>]   9.57M  25.8MB/s    in 0.4s    \n",
            "\n",
            "2025-02-24 22:25:35 (25.8 MB/s) - ‘flights.csv’ saved [10033769/10033769]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean\n",
        "\n",
        "# Crear sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"SimulacionStreaming\").getOrCreate()\n",
        "\n",
        "# Leer un CSV como si fuera un stream\n",
        "df = spark.read.option(\"header\", True).csv(\"/content/flights.csv\")\n",
        "\n",
        "# Simular un Streaming DataFrame con los datos necesarios\n",
        "retrasosStreamingDF = df.select(col(\"dest\"), col(\"arr_delay\").cast(\"float\"))\n",
        "\n",
        "# Definir la función retrasoMedio\n",
        "# Esta función calcula el retraso medio por destino\n",
        "def retrasoMedio(df):\n",
        "    return df.groupBy(\"dest\").agg(mean(\"arr_delay\").alias(\"retraso_medio\"))\n",
        "\n",
        "# Aplicar la función retrasoMedio\n",
        "resultado = retrasoMedio(retrasosStreamingDF)\n",
        "\n",
        "# Mostrar resultados\n",
        "resultado.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyngZqis32hN",
        "outputId": "e0652ab2-a497-4308-fa38-39ba67cce0f6"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------------+\n",
            "|dest|      retraso_medio|\n",
            "+----+-------------------+\n",
            "| MSY| -17.96039603960396|\n",
            "| GEG|  2.731818181818182|\n",
            "| SNA|-1.5201612903225807|\n",
            "| BUR|-1.6365357311878845|\n",
            "| EUG| 1.2041800643086817|\n",
            "| OAK| 10.067460317460318|\n",
            "| DCA| -4.000928505106778|\n",
            "| RDM| 2.5788732394366196|\n",
            "| KTN|   3.66692789968652|\n",
            "| LIH| -6.059011164274322|\n",
            "| IAH|-0.9413524835427888|\n",
            "| HNL|  -1.56978289765175|\n",
            "| CVG| -6.253164556962025|\n",
            "| SJC|  4.642902408111533|\n",
            "| AUS| -2.691588785046729|\n",
            "| LGB|-1.7124856815578464|\n",
            "| RNO|            5.55625|\n",
            "| JAC| 1.7857142857142858|\n",
            "| BOS| 0.5697230181470869|\n",
            "| EWR|  1.042455006922012|\n",
            "+----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el esquema del DataFrame\n",
        "retrasosStreamingDF.printSchema()"
      ],
      "metadata": {
        "id": "XVSpm9tp4lLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8394433-6a1f-43ee-818b-0ad5792a5a35"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dest: string (nullable = true)\n",
            " |-- arr_delay: float (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Asegurar que 'arr_delay' es de tipo float\n",
        "retrasosFinalDF = retrasosStreamingDF \\\n",
        "    .withColumn(\"arr_delay\", col(\"arr_delay\").cast(FloatType())) \\\n",
        "    .select(\"dest\", \"arr_delay\")\n",
        "\n",
        "# Mostrar el esquema para verificar que todo esté correcto\n",
        "retrasosFinalDF.printSchema()\n",
        "retrasosFinalDF.show(5)"
      ],
      "metadata": {
        "id": "CgyTjJGA4o2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Definir el esquema\n",
        "esquema = StructType([\n",
        "    StructField(\"dest\", StringType(), True),\n",
        "    StructField(\"arr_delay\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Transformar el DataFrame\n",
        "parsedDF = retrasosStreamingDF \\\n",
        "    .withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType())) \\\n",
        "    .select(\"dest\", \"arr_delay\")\n",
        "\n",
        "# Mostrar el esquema y los primeros datos\n",
        "parsedDF.printSchema()\n",
        "parsedDF.show(5)"
      ],
      "metadata": {
        "id": "1kFaLuru47FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Definir el esquema para la columna JSON \"value\"\n",
        "esquema = StructType([\n",
        "    StructField(\"dest\", StringType(), True),\n",
        "    StructField(\"arr_delay\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Simular la columna \"value\" convirtiendo el DataFrame a JSON en una nueva columna\n",
        "parsedDF = retrasosStreamingDF \\\n",
        "    .withColumn(\"value\", F.to_json(F.struct(F.col(\"dest\"), F.col(\"arr_delay\")))) \\\n",
        "    .withColumn(\"parejas\", F.from_json(F.col(\"value\"), esquema)) \\\n",
        "    .withColumn(\"dest\", F.col(\"parejas.dest\")) \\\n",
        "    .withColumn(\"arr_delay\", F.col(\"parejas.arr_delay\").cast(DoubleType()))\n",
        "\n",
        "# Verificar el esquema final\n",
        "parsedDF.printSchema()\n",
        "parsedDF.show(5)"
      ],
      "metadata": {
        "id": "zNi1clhB5Jcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tipos = parsedDF.dtypes\n",
        "assert((\"value\", \"string\") in tipos)\n",
        "assert(('parejas', 'struct<dest:string,arr_delay:double>') in tipos)\n",
        "assert(('dest', 'string') in tipos)\n",
        "assert(('arr_delay', 'double') in tipos)\n",
        "\n",
        "print(\"✅ Todas las aserciones pasaron correctamente.\")"
      ],
      "metadata": {
        "id": "lYziZMll5PWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simular un flujo de datos en lotes (batch processing)\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def simulacionStreaming(df: DataFrame):\n",
        "    print(\"📡 Simulando actualización de datos...\\n\")\n",
        "    retrasoMedioDF = retrasoMedio(df)\n",
        "    retrasoMedioDF.show()\n",
        "\n",
        "# Ejecutar la simulación\n",
        "simulacionStreaming(parsedDF)"
      ],
      "metadata": {
        "id": "oBv4OHHG5QYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "import time\n",
        "\n",
        "# Crear la sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"SimulacionStreaming\").getOrCreate()\n",
        "\n",
        "# Definir el esquema\n",
        "esquema = StructType([\n",
        "    StructField(\"dest\", StringType(), True),\n",
        "    StructField(\"arr_delay\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Datos simulados que llegarían desde Kafka\n",
        "mensajes_kafka = [\n",
        "    {\"dest\": \"GRX\", \"arr_delay\": 2.6},\n",
        "    {\"dest\": \"MAD\", \"arr_delay\": 5.4},\n",
        "    {\"dest\": \"GRX\", \"arr_delay\": 1.5},\n",
        "    {\"dest\": \"MAD\", \"arr_delay\": 20.0}\n",
        "]\n",
        "\n",
        "# Crear un DataFrame vacío para ir agregando los datos\n",
        "retrasosDF = spark.createDataFrame([], esquema)\n",
        "\n",
        "# Simulación de llegada de datos en streaming\n",
        "for mensaje in mensajes_kafka:\n",
        "    print(f\"\\n📡 Procesando nuevo mensaje: {mensaje}\")\n",
        "\n",
        "    # Crear un DataFrame temporal con el nuevo mensaje\n",
        "    nuevoDF = spark.createDataFrame([mensaje], esquema)\n",
        "\n",
        "    # Agregar el nuevo mensaje al DataFrame acumulado\n",
        "    retrasosDF = retrasosDF.union(nuevoDF)\n",
        "\n",
        "    # Aplicar la función retrasoMedio\n",
        "    retrasoMedioDF = retrasosDF.groupBy(\"dest\").agg(mean(\"arr_delay\").alias(\"retraso_medio\"))\n",
        "\n",
        "    # Mostrar resultados actualizados\n",
        "    retrasoMedioDF.show()\n",
        "\n",
        "    # Simular un pequeño retraso entre mensajes\n",
        "    time.sleep(3)"
      ],
      "metadata": {
        "id": "miMYGge857wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simular la consulta a una vista en memoria\n",
        "agregadosDF = retrasoMedioDF  # Usamos el DataFrame calculado en la simulación\n",
        "\n",
        "# Mostrar los resultados simulados\n",
        "agregadosDF.show()"
      ],
      "metadata": {
        "id": "qGYfMoJ06PG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que el DataFrame tiene las columnas correctas\n",
        "columnas = agregadosDF.columns\n",
        "\n",
        "# Aserciones para confirmar la estructura\n",
        "assert(len(columnas) == 2)\n",
        "assert(\"dest\" in columnas)\n",
        "assert(\"retraso_medio\" in columnas)\n",
        "\n",
        "print(\"✅ Todas las aserciones pasaron correctamente.\")"
      ],
      "metadata": {
        "id": "WbQsebGL6hbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agregadosDF.show()"
      ],
      "metadata": {
        "id": "jH8coXbw7X4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el retraso medio de GRX después del primer mensaje\n",
        "retraso_medio_GRX_primer_mensaje = agregadosDF.filter(agregadosDF.dest == \"GRX\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "# Mostrar el resultado capturado\n",
        "print(f\"📌 Retraso medio para GRX después del primer mensaje: {retraso_medio_GRX_primer_mensaje}\")"
      ],
      "metadata": {
        "id": "aW4-Jx2U7khQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir el nuevo mensaje con el DataFrame acumulado\n",
        "retrasosDF = retrasosDF.union(nuevoDF)"
      ],
      "metadata": {
        "id": "EIFOFaBI7wDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalcular el retraso medio después de agregar nuevos datos\n",
        "agregadosDF = retrasosDF.groupBy(\"dest\").agg(mean(\"arr_delay\").alias(\"retraso_medio\"))"
      ],
      "metadata": {
        "id": "gr8qI2Dn8he4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar nuevos datos al DataFrame\n",
        "retrasosDF = retrasosDF.union(nuevoDF)\n",
        "\n",
        "# Recalcular agregados\n",
        "agregadosDF = retrasosDF.groupBy(\"dest\").agg(mean(\"arr_delay\").alias(\"retraso_medio\"))\n",
        "\n",
        "# Mostrar los datos actualizados\n",
        "agregadosDF.show()"
      ],
      "metadata": {
        "id": "x1-zKm1-71H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los retrasos medios de GRX y MAD después del segundo mensaje\n",
        "retraso_medio_GRX_segundo_mensaje = agregadosDF.filter(agregadosDF.dest == \"GRX\") \\\n",
        "                                               .select(\"retraso_medio\") \\\n",
        "                                               .collect()[0][0]\n",
        "\n",
        "retraso_medio_MAD_segundo_mensaje = agregadosDF.filter(agregadosDF.dest == \"MAD\") \\\n",
        "                                               .select(\"retraso_medio\") \\\n",
        "                                               .collect()[0][0]\n",
        "\n",
        "print(f\"📌 Retraso medio para GRX después del segundo mensaje: {retraso_medio_GRX_segundo_mensaje}\")\n",
        "print(f\"📌 Retraso medio para MAD después del segundo mensaje: {retraso_medio_MAD_segundo_mensaje}\")"
      ],
      "metadata": {
        "id": "VdxfQuEE8yX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el retraso medio de GRX después del primer mensaje\n",
        "retraso_medio_GRX_primer_mensaje = agregadosDF.filter(agregadosDF.dest == \"GRX\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "# Mostrar el resultado capturado\n",
        "print(f\"📌 Retraso medio para GRX después del primer mensaje: {retraso_medio_GRX_primer_mensaje}\")"
      ],
      "metadata": {
        "id": "ny875AOA9fPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir el nuevo mensaje con el DataFrame acumulado\n",
        "retrasosDF = retrasosDF.union(nuevoDF)"
      ],
      "metadata": {
        "id": "frQkH8369zqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalcular el retraso medio después de agregar nuevos datos\n",
        "agregadosDF = retrasosDF.groupBy(\"dest\").agg(mean(\"arr_delay\").alias(\"retraso_medio\"))"
      ],
      "metadata": {
        "id": "EiRRUvli92Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agregadosDF.show()"
      ],
      "metadata": {
        "id": "4xLSqodl94ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los retrasos medios después del tercer mensaje\n",
        "retraso_medio_GRX_tercer_mensaje = agregadosDF.filter(agregadosDF.dest == \"GRX\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "retraso_medio_MAD_tercer_mensaje = agregadosDF.filter(agregadosDF.dest == \"MAD\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "print(f\"📌 Retraso medio para GRX después del tercer mensaje: {retraso_medio_GRX_tercer_mensaje}\")\n",
        "print(f\"📌 Retraso medio para MAD después del tercer mensaje: {retraso_medio_MAD_tercer_mensaje}\")"
      ],
      "metadata": {
        "id": "sDlzY88j991E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el retraso medio de GRX después del primer mensaje\n",
        "retraso_medio_GRX_primer_mensaje = agregadosDF.filter(agregadosDF.dest == \"GRX\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "# Mostrar el resultado capturado\n",
        "print(f\"📌 Retraso medio para GRX después del primer mensaje: {retraso_medio_GRX_primer_mensaje}\")"
      ],
      "metadata": {
        "id": "3PbULnzO-NrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir el nuevo mensaje con el DataFrame acumulado\n",
        "retrasosDF = retrasosDF.union(nuevoDF)"
      ],
      "metadata": {
        "id": "IGHV-EaR-QT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalcular el retraso medio después de agregar nuevos datos\n",
        "agregadosDF = retrasosDF.groupBy(\"dest\").agg(mean(\"arr_delay\").alias(\"retraso_medio\"))"
      ],
      "metadata": {
        "id": "4mZQZBZC-Sag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los retrasos medios después del tercer mensaje\n",
        "retraso_medio_GRX_cuarto_mensaje = agregadosDF.filter(agregadosDF.dest == \"GRX\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "retraso_medio_MAD_cuarto_mensaje = agregadosDF.filter(agregadosDF.dest == \"MAD\") \\\n",
        "                                              .select(\"retraso_medio\") \\\n",
        "                                              .collect()[0][0]\n",
        "\n",
        "print(f\"📌 Retraso medio para GRX después del tercer mensaje: {retraso_medio_GRX_tercer_mensaje}\")\n",
        "print(f\"📌 Retraso medio para MAD después del tercer mensaje: {retraso_medio_MAD_tercer_mensaje}\")"
      ],
      "metadata": {
        "id": "atZfSBY1-TpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}